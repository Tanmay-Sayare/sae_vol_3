{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4672f0b1-2215-4607-897a-ca0b093342c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.17.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib seaborn tensorflow nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d2648-4bf2-4c7b-9a6a-22e0711f9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjective Answer Evaluation System\n",
    "# This notebook implements a deep learning model to evaluate subjective answers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data(file_path=None):\n",
    "    \"\"\"\n",
    "    Load data from CSV file or use sample data if file_path is None\n",
    "    \"\"\"\n",
    "    if file_path:\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # Create sample dataframe based on the provided example\n",
    "        data = {\n",
    "            'id': [1.1, 1.1, 1.1, 1.1, 1.1, 1.1],\n",
    "            'Question': ['What is the role of a prototype?' for _ in range(6)],\n",
    "            'Desired_answer': ['To simulate the behavior of the actual system.' for _ in range(6)],\n",
    "            'Student_answer': [\n",
    "                'High risk problems can be detected.',\n",
    "                'To simulate portions of the software which are yet to be built.',\n",
    "                'A prototype program helps to discover requirements.',\n",
    "                'Defined in the Specification document.',\n",
    "                'It is used to let the user evaluate developer proposals.',\n",
    "                'To find problems and solutions.'\n",
    "            ],\n",
    "            'score_me': [4, 5, 5, 5, 3, 2],\n",
    "            'score_other': [3, 5, 3, 5, 3, 2],\n",
    "            'score_avg': [3.5, 5, 4, 5, 3, 2]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Text preprocessing functions\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by lowercasing, removing punctuation, stopwords, and lemmatizing\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Feature engineering\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract features for model training\"\"\"\n",
    "    \n",
    "    # Preprocess text columns\n",
    "    df['processed_question'] = df['Question'].apply(preprocess_text)\n",
    "    df['processed_desired_answer'] = df['Desired_answer'].apply(preprocess_text)\n",
    "    df['processed_student_answer'] = df['Student_answer'].apply(preprocess_text)\n",
    "    \n",
    "    # Calculate text length features\n",
    "    df['student_answer_length'] = df['Student_answer'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "    df['desired_answer_length'] = df['Desired_answer'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "    df['length_difference'] = abs(df['student_answer_length'] - df['desired_answer_length'])\n",
    "    \n",
    "    # Calculate cosine similarity between student answer and desired answer\n",
    "    def get_cosine_sim(text1, text2):\n",
    "        if not isinstance(text1, str) or not isinstance(text2, str):\n",
    "            return 0\n",
    "        \n",
    "        # Tokenize and create term frequency vectors\n",
    "        all_words = set(text1.split() + text2.split())\n",
    "        vec1 = [text1.split().count(word) for word in all_words]\n",
    "        vec2 = [text2.split().count(word) for word in all_words]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        vec1 = np.array(vec1).reshape(1, -1)\n",
    "        vec2 = np.array(vec2).reshape(1, -1)\n",
    "        \n",
    "        if np.sum(vec1) == 0 or np.sum(vec2) == 0:\n",
    "            return 0\n",
    "            \n",
    "        return cosine_similarity(vec1, vec2)[0][0]\n",
    "    \n",
    "    df['cosine_similarity'] = df.apply(\n",
    "        lambda row: get_cosine_sim(row['processed_student_answer'], row['processed_desired_answer']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Use 'score_avg' as the target variable\n",
    "    df['target_score'] = df['score_avg']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Tokenize and prepare sequences\n",
    "def prepare_sequences(df, max_words=5000, max_seq_length=100):\n",
    "    \"\"\"Tokenize and prepare sequences for LSTM model\"\"\"\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(df['processed_student_answer'].tolist() + df['processed_desired_answer'].tolist())\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    student_sequences = tokenizer.texts_to_sequences(df['processed_student_answer'])\n",
    "    desired_sequences = tokenizer.texts_to_sequences(df['processed_desired_answer'])\n",
    "    \n",
    "    # Pad sequences\n",
    "    student_padded = pad_sequences(student_sequences, maxlen=max_seq_length, padding='post')\n",
    "    desired_padded = pad_sequences(desired_sequences, maxlen=max_seq_length, padding='post')\n",
    "    \n",
    "    # Save tokenizer\n",
    "    with open('answer_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return student_padded, desired_padded, tokenizer\n",
    "\n",
    "# Build LSTM model\n",
    "def build_model(tokenizer, max_seq_length=100, embedding_dim=128):\n",
    "    \"\"\"Build a Bidirectional LSTM model with regularization\"\"\"\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    model = Sequential([\n",
    "        # First input branch for student answer\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_seq_length),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001))),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(32, kernel_regularizer=l2(0.001))),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Compile model with Adam optimizer and learning rate\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, epochs=50, batch_size=32):\n",
    "    \"\"\"Train the model with early stopping and checkpointing\"\"\"\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate the model and print performance metrics\"\"\"\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Round predictions to nearest 0.5\n",
    "    y_pred_rounded = np.round(y_pred * 2) / 2\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    print(f\"RÂ² Score: {r2:.4f}\")\n",
    "    \n",
    "    return mse, mae, r2, y_pred, y_pred_rounded\n",
    "\n",
    "# Visualize results\n",
    "def visualize_results(history, y_test, y_pred, y_pred_rounded):\n",
    "    \"\"\"Visualize training history and prediction results\"\"\"\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Training and Validation MAE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
    "    plt.title('Actual vs Predicted Scores')\n",
    "    plt.xlabel('Actual Score')\n",
    "    plt.ylabel('Predicted Score')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, y_pred_rounded, alpha=0.5)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
    "    plt.title('Actual vs Rounded Predicted Scores')\n",
    "    plt.xlabel('Actual Score')\n",
    "    plt.ylabel('Rounded Predicted Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot score distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(y_test, bins=9, alpha=0.5, label='Actual', kde=True)\n",
    "    sns.histplot(y_pred_rounded, bins=9, alpha=0.5, label='Predicted', kde=True)\n",
    "    plt.title('Score Distribution')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(data=pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()}).melt())\n",
    "    plt.title('Score Distributions (Box Plot)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('score_distribution.png')\n",
    "    plt.show()\n",
    "\n",
    "# Save model and prediction\n",
    "def save_model_and_results(model, tokenizer, df, y_pred, y_pred_rounded):\n",
    "    \"\"\"Save the model, tokenizer, and prediction results\"\"\"\n",
    "    \n",
    "    # Save model\n",
    "    model.save('subjective_answer_evaluation.keras')\n",
    "    \n",
    "    # Save tokenizer\n",
    "    with open('answer_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Save predictions\n",
    "    results_df = df.copy()\n",
    "    results_df['predicted_score'] = y_pred\n",
    "    results_df['predicted_score_rounded'] = y_pred_rounded\n",
    "    results_df['error'] = abs(results_df['target_score'] - results_df['predicted_score_rounded'])\n",
    "    \n",
    "    results_df.to_csv('prediction_results.csv', index=False)\n",
    "    \n",
    "    print(\"Model saved as 'subjective_answer_evaluation.keras'\")\n",
    "    print(\"Tokenizer saved as 'answer_tokenizer.pickle'\")\n",
    "    print(\"Predictions saved as 'prediction_results.csv'\")\n",
    "\n",
    "# Function to create a prediction pipeline\n",
    "def create_prediction_pipeline(saved_model_path, tokenizer_path, max_seq_length=100):\n",
    "    \"\"\"Create a pipeline for making predictions on new data\"\"\"\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model = load_model(saved_model_path)\n",
    "    \n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    \n",
    "    def predict_score(question, desired_answer, student_answer):\n",
    "        # Preprocess texts\n",
    "        processed_question = preprocess_text(question)\n",
    "        processed_desired_answer = preprocess_text(desired_answer)\n",
    "        processed_student_answer = preprocess_text(student_answer)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        all_words = set(processed_desired_answer.split() + processed_student_answer.split())\n",
    "        if all_words:\n",
    "            vec1 = [processed_desired_answer.split().count(word) for word in all_words]\n",
    "            vec2 = [processed_student_answer.split().count(word) for word in all_words]\n",
    "            \n",
    "            vec1 = np.array(vec1).reshape(1, -1)\n",
    "            vec2 = np.array(vec2).reshape(1, -1)\n",
    "            \n",
    "            if np.sum(vec1) > 0 and np.sum(vec2) > 0:\n",
    "                cosine_sim = cosine_similarity(vec1, vec2)[0][0]\n",
    "            else:\n",
    "                cosine_sim = 0\n",
    "        else:\n",
    "            cosine_sim = 0\n",
    "        \n",
    "        # Convert to sequences and pad\n",
    "        student_seq = tokenizer.texts_to_sequences([processed_student_answer])\n",
    "        student_padded = pad_sequences(student_seq, maxlen=max_seq_length, padding='post')\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(student_padded)[0][0]\n",
    "        \n",
    "        # Round to nearest 0.5\n",
    "        rounded_prediction = round(prediction * 2) / 2\n",
    "        \n",
    "        # Ensure prediction is in valid range (1-5)\n",
    "        rounded_prediction = max(1, min(5, rounded_prediction))\n",
    "        \n",
    "        return {\n",
    "            'original_prediction': float(prediction),\n",
    "            'rounded_prediction': float(rounded_prediction),\n",
    "            'cosine_similarity': float(cosine_sim)\n",
    "        }\n",
    "    \n",
    "    return predict_score\n",
    "\n",
    "# Main function to execute the entire workflow\n",
    "def main():\n",
    "    # Load data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = load_data()\n",
    "    \n",
    "    # Extract features\n",
    "    df = extract_features(df)\n",
    "    \n",
    "    # Prepare sequences\n",
    "    print(\"Preparing sequences...\")\n",
    "    student_padded, desired_padded, tokenizer = prepare_sequences(df)\n",
    "    \n",
    "    # Extract additional features (optional)\n",
    "    additional_features = df[['cosine_similarity', 'length_difference']].values\n",
    "    \n",
    "    # Combine sequences and additional features\n",
    "    X = student_padded\n",
    "    y = df['target_score'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Build model\n",
    "    print(\"Building model...\")\n",
    "    model = build_model(tokenizer)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    history, trained_model = train_model(model, X_train, y_train, X_valid, y_valid)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    mse, mae, r2, y_pred, y_pred_rounded = evaluate_model(trained_model, X_test, y_test)\n",
    "    \n",
    "    # Visualize results\n",
    "    print(\"Visualizing results...\")\n",
    "    visualize_results(history, y_test, y_pred, y_pred_rounded)\n",
    "    \n",
    "    # Save model and results\n",
    "    print(\"Saving model and results...\")\n",
    "    save_model_and_results(trained_model, tokenizer, df, y_pred, y_pred_rounded)\n",
    "    \n",
    "    # Create prediction pipeline\n",
    "    print(\"Creating prediction pipeline...\")\n",
    "    predict_score = create_prediction_pipeline('subjective_answer_evaluation.keras', 'answer_tokenizer.pickle')\n",
    "    \n",
    "    # Example prediction\n",
    "    example_prediction = predict_score(\n",
    "        \"What is the role of a prototype?\",\n",
    "        \"To simulate the behavior of the actual system.\",\n",
    "        \"A prototype helps in identifying potential issues in the system design.\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nExample prediction:\")\n",
    "    print(f\"Question: What is the role of a prototype?\")\n",
    "    print(f\"Desired answer: To simulate the behavior of the actual system.\")\n",
    "    print(f\"Student answer: A prototype helps in identifying potential issues in the system design.\")\n",
    "    print(f\"Predicted score: {example_prediction['rounded_prediction']}/5\")\n",
    "    print(f\"Cosine similarity: {example_prediction['cosine_similarity']:.4f}\")\n",
    "    \n",
    "    print(\"\\nModel training and evaluation completed successfully.\")\n",
    "\n",
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a19848-56df-4eb0-bc62-9607e7eee8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1ce85-ac29-47c6-ae03-9463b3c12e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde760a-36bd-49d1-b511-15f8891d80a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec7898-4d8e-47ed-90d9-da2e363814a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c498a-2ac0-47de-8375-fd0f27170c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000449c-0458-4fe6-9b2b-941a4c8a3bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275add6-b750-4ff7-8344-01acec2e1151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
