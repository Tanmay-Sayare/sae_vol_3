{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafca6fe-0655-4bd9-bdff-1d71e1435f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (2.17.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    " !pip install pandas numpy matplotlib seaborn tensorflow nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2798fdc-5a76-4687-ae72-34ae7166a12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader punkt stopwords wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4832ce9-7d8d-4dd7-8418-498c1b36974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de09f7c8-1e28-45f6-b3ed-36f23809016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc1fd8b-155b-40cb-95ab-58484d21fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "def load_data(file_path='new_dataset.csv'):\n",
    "    \"\"\"\n",
    "    Load data from CSV file\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded dataset with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"\\nMissing values in the dataset:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "        \n",
    "        # Handle missing values\n",
    "        print(\"Handling missing values...\")\n",
    "        df = df.dropna(subset=['Question', 'Desired_answer', 'Student_answer', 'score_avg'])\n",
    "        print(f\"Dataset after handling missing values: {df.shape[0]} rows\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a30145cd-768f-4fa0-bd80-64d51ef14420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by lowercasing, removing punctuation, stopwords, and lemmatizing\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87a4faf0-f8ad-4d7f-9d98-69eb1606ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract features for model training\"\"\"\n",
    "    \n",
    "    print(\"Extracting features...\")\n",
    "    \n",
    "    # Preprocess text columns\n",
    "    print(\"Preprocessing text columns...\")\n",
    "    df['processed_question'] = df['Question'].apply(preprocess_text)\n",
    "    df['processed_desired_answer'] = df['Desired_answer'].apply(preprocess_text)\n",
    "    df['processed_student_answer'] = df['Student_answer'].apply(preprocess_text)\n",
    "    \n",
    "    # Calculate text length features\n",
    "    print(\"Calculating text length features...\")\n",
    "    df['student_answer_length'] = df['Student_answer'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "    df['desired_answer_length'] = df['Desired_answer'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "    df['length_difference'] = abs(df['student_answer_length'] - df['desired_answer_length'])\n",
    "    \n",
    "    # Calculate word overlap\n",
    "    def word_overlap_ratio(text1, text2):\n",
    "        if not isinstance(text1, str) or not isinstance(text2, str):\n",
    "            return 0\n",
    "        \n",
    "        words1 = set(text1.split())\n",
    "        words2 = set(text2.split())\n",
    "        \n",
    "        if len(words1) == 0 or len(words2) == 0:\n",
    "            return 0\n",
    "            \n",
    "        overlap = len(words1.intersection(words2))\n",
    "        return overlap / max(len(words1), len(words2))\n",
    "    \n",
    "    print(\"Calculating word overlap ratio...\")\n",
    "    df['word_overlap_ratio'] = df.apply(\n",
    "        lambda row: word_overlap_ratio(row['processed_student_answer'], row['processed_desired_answer']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate cosine similarity between student answer and desired answer\n",
    "    def get_cosine_sim(text1, text2):\n",
    "        if not isinstance(text1, str) or not isinstance(text2, str):\n",
    "            return 0\n",
    "        \n",
    "        # Tokenize and create term frequency vectors\n",
    "        all_words = set(text1.split() + text2.split())\n",
    "        \n",
    "        if not all_words:\n",
    "            return 0\n",
    "            \n",
    "        vec1 = [text1.split().count(word) for word in all_words]\n",
    "        vec2 = [text2.split().count(word) for word in all_words]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        vec1 = np.array(vec1).reshape(1, -1)\n",
    "        vec2 = np.array(vec2).reshape(1, -1)\n",
    "        \n",
    "        if np.sum(vec1) == 0 or np.sum(vec2) == 0:\n",
    "            return 0\n",
    "            \n",
    "        return cosine_similarity(vec1, vec2)[0][0]\n",
    "    \n",
    "    print(\"Calculating cosine similarity...\")\n",
    "    df['cosine_similarity'] = df.apply(\n",
    "        lambda row: get_cosine_sim(row['processed_student_answer'], row['processed_desired_answer']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Use 'score_avg' as the target variable\n",
    "    df['target_score'] = df['score_avg']\n",
    "    \n",
    "    # Print feature statistics\n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    print(df[['student_answer_length', 'desired_answer_length', 'length_difference', \n",
    "              'word_overlap_ratio', 'cosine_similarity', 'target_score']].describe())\n",
    "    \n",
    "    # Create a correlation heatmap\n",
    "    corr = df[['student_answer_length', 'desired_answer_length', 'length_difference', \n",
    "              'word_overlap_ratio', 'cosine_similarity', 'target_score']].corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_correlation.png')\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51d9e304-5e02-4d3c-a735-a24f9b8911f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and prepare sequences\n",
    "def prepare_sequences(df, max_words=10000, max_seq_length=150):\n",
    "    \"\"\"Tokenize and prepare sequences for LSTM model\"\"\"\n",
    "    \n",
    "    print(f\"Preparing sequences with max_words={max_words}, max_seq_length={max_seq_length}...\")\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(df['processed_student_answer'].tolist() + df['processed_desired_answer'].tolist())\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(tokenizer.word_index) + 1}\")\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    student_sequences = tokenizer.texts_to_sequences(df['processed_student_answer'])\n",
    "    desired_sequences = tokenizer.texts_to_sequences(df['processed_desired_answer'])\n",
    "    \n",
    "    # Pad sequences\n",
    "    student_padded = pad_sequences(student_sequences, maxlen=max_seq_length, padding='post')\n",
    "    desired_padded = pad_sequences(desired_sequences, maxlen=max_seq_length, padding='post')\n",
    "    \n",
    "    # Save tokenizer\n",
    "    with open('answer_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print(\"Tokenizer saved as 'answer_tokenizer.pickle'\")\n",
    "    \n",
    "    # Prepare additional features\n",
    "    additional_features = df[['cosine_similarity', 'word_overlap_ratio', 'length_difference']].values\n",
    "    \n",
    "    return student_padded, desired_padded, additional_features, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8557bd8-3d5a-4805-84c9-44bedcc46e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build enhanced LSTM model\n",
    "def build_model(tokenizer, max_seq_length=150, embedding_dim=200, include_additional_features=True, num_additional_features=3):\n",
    "    \"\"\"Build a Bidirectional LSTM model with regularization and additional features\"\"\"\n",
    "    \n",
    "    vocab_size = min(len(tokenizer.word_index) + 1, 10000)  # Limit vocab size\n",
    "    \n",
    "    # Input for student answer\n",
    "    student_input = tf.keras.Input(shape=(max_seq_length,), name='student_input')\n",
    "    \n",
    "    # Embedding and LSTM layers for student answer\n",
    "    embedding = Embedding(vocab_size, embedding_dim, input_length=max_seq_length)(student_input)\n",
    "    dropout1 = Dropout(0.3)(embedding)\n",
    "    bilstm1 = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001)))(dropout1)\n",
    "    dropout2 = Dropout(0.3)(bilstm1)\n",
    "    bilstm2 = Bidirectional(LSTM(64, kernel_regularizer=l2(0.001)))(dropout2)\n",
    "    dropout3 = Dropout(0.3)(bilstm2)\n",
    "    \n",
    "    # Include additional handcrafted features if specified\n",
    "    if include_additional_features:\n",
    "        additional_input = tf.keras.Input(shape=(num_additional_features,), name='additional_features')\n",
    "        \n",
    "        # Concatenate LSTM output with additional features\n",
    "        concatenated = tf.keras.layers.concatenate([dropout3, additional_input])\n",
    "        \n",
    "        # Dense layers\n",
    "        dense1 = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(concatenated)\n",
    "        dropout4 = Dropout(0.3)(dense1)\n",
    "        dense2 = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(dropout4)\n",
    "        dropout5 = Dropout(0.2)(dense2)\n",
    "        dense3 = Dense(16, activation='relu', kernel_regularizer=l2(0.001))(dropout5)\n",
    "        \n",
    "        # Output layer\n",
    "        output = Dense(1, activation='linear')(dense3)\n",
    "        \n",
    "        # Create model with both inputs\n",
    "        model = tf.keras.Model(inputs=[student_input, additional_input], outputs=output)\n",
    "    else:\n",
    "        # Dense layers\n",
    "        dense1 = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(dropout3)\n",
    "        dropout4 = Dropout(0.3)(dense1)\n",
    "        dense2 = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(dropout4)\n",
    "        dropout5 = Dropout(0.2)(dense2)\n",
    "        dense3 = Dense(16, activation='relu', kernel_regularizer=l2(0.001))(dropout5)\n",
    "        \n",
    "        # Output layer\n",
    "        output = Dense(1, activation='linear')(dense3)\n",
    "        \n",
    "        # Create model with just student input\n",
    "        model = tf.keras.Model(inputs=student_input, outputs=output)\n",
    "    \n",
    "    # Compile model with Adam optimizer and learning rate\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7cc5032-6b7d-4509-91d4-b50d03676ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, epochs=50, batch_size=32):\n",
    "    \"\"\"Train the model with early stopping and checkpointing\"\"\"\n",
    "    \n",
    "    print(f\"Training model with {epochs} epochs and batch size {batch_size}...\")\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Reduce learning rate when plateau\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd3441ce-9d6d-43e5-8d2e-270803ec7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate the model and print performance metrics\"\"\"\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Round predictions to nearest 0.5\n",
    "    y_pred_rounded = np.round(y_pred * 2) / 2\n",
    "    \n",
    "    # Ensure predictions are in valid range (1-5)\n",
    "    y_pred_rounded = np.clip(y_pred_rounded, 1, 5)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Calculate accuracy with different tolerances\n",
    "    tolerance_05 = np.mean(abs(y_test - y_pred_rounded) <= 0.5)\n",
    "    tolerance_10 = np.mean(abs(y_test - y_pred_rounded) <= 1.0)\n",
    "    \n",
    "    print(f\"Accuracy (±0.5 points): {tolerance_05:.4f}\")\n",
    "    print(f\"Accuracy (±1.0 points): {tolerance_10:.4f}\")\n",
    "    \n",
    "    return mse, mae, r2, y_pred, y_pred_rounded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0acebfd1-a7aa-4728-9305-545ba76915c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize results\n",
    "def visualize_results(history, y_test, y_pred, y_pred_rounded):\n",
    "    \"\"\"Visualize training history and prediction results\"\"\"\n",
    "    \n",
    "    print(\"Visualizing results...\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Train MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Training and Validation MAE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
    "    plt.title('Actual vs Predicted Scores')\n",
    "    plt.xlabel('Actual Score')\n",
    "    plt.ylabel('Predicted Score')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, y_pred_rounded, alpha=0.5)\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
    "    plt.title('Actual vs Rounded Predicted Scores')\n",
    "    plt.xlabel('Actual Score')\n",
    "    plt.ylabel('Rounded Predicted Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot prediction error distribution\n",
    "    error = y_test - y_pred_rounded\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(error, bins=15, kde=True)\n",
    "    plt.title('Prediction Error Distribution')\n",
    "    plt.xlabel('Error (Actual - Predicted)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('error_distribution.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot score distribution\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(y_test, bins=9, alpha=0.5, label='Actual', kde=True)\n",
    "    sns.histplot(y_pred_rounded, bins=9, alpha=0.5, label='Predicted', kde=True)\n",
    "    plt.title('Score Distribution')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(data=pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()}).melt())\n",
    "    plt.title('Score Distributions (Box Plot)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('score_distribution.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f466fe2f-9844-4daf-b57f-67eb6a3eb922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model and prediction\n",
    "def save_model_and_results(model, tokenizer, df, y_pred, y_pred_rounded):\n",
    "    \"\"\"Save the model, tokenizer, and prediction results\"\"\"\n",
    "    \n",
    "    print(\"Saving model and results...\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save('subjective_answer_evaluation.keras')\n",
    "    \n",
    "    # Save tokenizer\n",
    "    with open('answer_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Save predictions\n",
    "    results_df = df.copy()\n",
    "    results_df['predicted_score'] = y_pred\n",
    "    results_df['predicted_score_rounded'] = y_pred_rounded\n",
    "    results_df['error'] = abs(results_df['target_score'] - results_df['predicted_score_rounded'])\n",
    "    \n",
    "    results_df.to_csv('prediction_results.csv', index=False)\n",
    "    \n",
    "    print(\"Model saved as 'subjective_answer_evaluation.keras'\")\n",
    "    print(\"Tokenizer saved as 'answer_tokenizer.pickle'\")\n",
    "    print(\"Predictions saved as 'prediction_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edf2051f-cc13-41ab-af7a-1646b3c424c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to create a prediction pipeline\n",
    "def create_prediction_pipeline(saved_model_path, tokenizer_path, max_seq_length=150, include_additional_features=True):\n",
    "    \"\"\"Create a pipeline for making predictions on new data\"\"\"\n",
    "    \n",
    "    print(\"Creating prediction pipeline...\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model = load_model(saved_model_path)\n",
    "    \n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    \n",
    "    def predict_score(question, desired_answer, student_answer):\n",
    "        # Preprocess texts\n",
    "        processed_question = preprocess_text(question)\n",
    "        processed_desired_answer = preprocess_text(desired_answer)\n",
    "        processed_student_answer = preprocess_text(student_answer)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        all_words = set(processed_desired_answer.split() + processed_student_answer.split())\n",
    "        if all_words:\n",
    "            vec1 = [processed_desired_answer.split().count(word) for word in all_words]\n",
    "            vec2 = [processed_student_answer.split().count(word) for word in all_words]\n",
    "            \n",
    "            vec1 = np.array(vec1).reshape(1, -1)\n",
    "            vec2 = np.array(vec2).reshape(1, -1)\n",
    "            \n",
    "            if np.sum(vec1) > 0 and np.sum(vec2) > 0:\n",
    "                cosine_sim = cosine_similarity(vec1, vec2)[0][0]\n",
    "            else:\n",
    "                cosine_sim = 0\n",
    "        else:\n",
    "            cosine_sim = 0\n",
    "        \n",
    "        # Calculate word overlap ratio\n",
    "        words1 = set(processed_desired_answer.split())\n",
    "        words2 = set(processed_student_answer.split())\n",
    "        \n",
    "        if len(words1) > 0 and len(words2) > 0:\n",
    "            overlap = len(words1.intersection(words2))\n",
    "            word_overlap = overlap / max(len(words1), len(words2))\n",
    "        else:\n",
    "            word_overlap = 0\n",
    "        \n",
    "        # Calculate length difference\n",
    "        len_diff = abs(len(processed_desired_answer.split()) - len(processed_student_answer.split()))\n",
    "        \n",
    "        # Convert to sequences and pad\n",
    "        student_seq = tokenizer.texts_to_sequences([processed_student_answer])\n",
    "        student_padded = pad_sequences(student_seq, maxlen=max_seq_length, padding='post')\n",
    "        \n",
    "        # Create additional features array\n",
    "        additional_features = np.array([[cosine_sim, word_overlap, len_diff]])\n",
    "        \n",
    "        # Make prediction\n",
    "        if include_additional_features:\n",
    "            prediction = model.predict([student_padded, additional_features])[0][0]\n",
    "        else:\n",
    "            prediction = model.predict(student_padded)[0][0]\n",
    "        \n",
    "        # Round to nearest 0.5\n",
    "        rounded_prediction = round(prediction * 2) / 2\n",
    "        \n",
    "        # Ensure prediction is in valid range (1-5)\n",
    "        rounded_prediction = max(1, min(5, rounded_prediction))\n",
    "        \n",
    "        return {\n",
    "            'original_prediction': float(prediction),\n",
    "            'rounded_prediction': float(rounded_prediction),\n",
    "            'cosine_similarity': float(cosine_sim),\n",
    "            'word_overlap_ratio': float(word_overlap),\n",
    "            'feature_importance': {\n",
    "                'cosine_similarity': 'High correlation with scoring',\n",
    "                'word_overlap_ratio': 'Medium correlation with scoring',\n",
    "                'length_difference': 'Lower correlation with scoring'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return predict_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63675545-15cc-40c2-b43a-a46132bd10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to execute the entire workflow\n",
    "def main():\n",
    "    # Load data - specify your actual dataset file here\n",
    "    print(\"Starting subjective answer evaluation system...\")\n",
    "    df = load_data('subjective_answers_dataset.csv')\n",
    "    \n",
    "    # Extract features\n",
    "    df = extract_features(df)\n",
    "    \n",
    "    # Prepare sequences and additional features\n",
    "    student_padded, desired_padded, additional_features, tokenizer = prepare_sequences(df)\n",
    "    \n",
    "    # Prepare inputs based on model type\n",
    "    include_additional_features = True  # Set to True to use additional handcrafted features\n",
    "    \n",
    "    if include_additional_features:\n",
    "        X = [student_padded, additional_features]\n",
    "    else:\n",
    "        X = student_padded\n",
    "        \n",
    "    y = df['target_score'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Split data\n",
    "    # For larger datasets, consider using a smaller test/validation ratio\n",
    "    test_size = 0.15\n",
    "    print(f\"Splitting data with test_size={test_size}...\")\n",
    "    \n",
    "    if include_additional_features:\n",
    "        # Split for multi-input model\n",
    "        X_student_train, X_student_temp, X_features_train, X_features_temp, y_train, y_temp = train_test_split(\n",
    "            student_padded, additional_features, y, test_size=test_size*2, random_state=42\n",
    "        )\n",
    "        \n",
    "        X_student_valid, X_student_test, X_features_valid, X_features_test, y_valid, y_test = train_test_split(\n",
    "            X_student_temp, X_features_temp, y_temp, test_size=0.5, random_state=42\n",
    "        )\n",
    "        \n",
    "        X_train = [X_student_train, X_features_train]\n",
    "        X_valid = [X_student_valid, X_features_valid]\n",
    "        X_test = [X_student_test, X_features_test]\n",
    "    else:\n",
    "        # Split for single-input model\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size*2, random_state=42)\n",
    "        X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Training set size: {len(y_train)}\")\n",
    "    print(f\"Validation set size: {len(y_valid)}\")\n",
    "    print(f\"Test set size: {len(y_test)}\")\n",
    "    \n",
    "    # Build model\n",
    "    print(\"Building model...\")\n",
    "    model = build_model(tokenizer, \n",
    "                      max_seq_length=student_padded.shape[1], \n",
    "                      include_additional_features=include_additional_features,\n",
    "                      num_additional_features=additional_features.shape[1] if include_additional_features else 0)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    history, trained_model = train_model(model, X_train, y_train, X_valid, y_valid, epochs=100, batch_size=64)\n",
    "    \n",
    "    # Evaluate model\n",
    "    mse, mae, r2, y_pred, y_pred_rounded = evaluate_model(trained_model, X_test, y_test)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results(history, y_test, y_pred, y_pred_rounded)\n",
    "    \n",
    "    # Save model and results\n",
    "    save_model_and_results(trained_model, tokenizer, df.iloc[y_test.shape[0]:], y_pred, y_pred_rounded)\n",
    "    \n",
    "    # Create prediction pipeline\n",
    "    predict_score = create_prediction_pipeline('subjective_answer_evaluation.keras', \n",
    "                                            'answer_tokenizer.pickle',\n",
    "                                            include_additional_features=include_additional_features)\n",
    "    \n",
    "    # Example prediction\n",
    "    example_prediction = predict_score(\n",
    "        \"What is the role of a prototype?\",\n",
    "        \"To simulate the behavior of the actual system.\",\n",
    "        \"A prototype helps in identifying potential issues in the system design.\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nExample prediction:\")\n",
    "    print(f\"Question: What is the role of a prototype?\")\n",
    "    print(f\"Desired answer: To simulate the behavior of the actual system.\")\n",
    "    print(f\"Student answer: A prototype helps in identifying potential issues in the system design.\")\n",
    "    print(f\"Predicted score: {example_prediction['rounded_prediction']}/5\")\n",
    "    print(f\"Cosine similarity: {example_prediction['cosine_similarity']:.4f}\")\n",
    "    \n",
    "    print(\"\\nModel training and evaluation completed successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba7e40d5-bab3-4822-aaee-528f3b6f40bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting subjective answer evaluation system...\n",
      "Loading data from subjective_answers_dataset.csv...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'subjective_answers_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Download required NLTK resources\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# !python -m nltk.downloader punkt stopwords wordnet\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Execute main function\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 8\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Load data - specify your actual dataset file here\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting subjective answer evaluation system...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     df \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubjective_answers_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     df \u001b[38;5;241m=\u001b[39m extract_features(df)\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mLoad data from CSV file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Display dataset info\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'subjective_answers_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "# !python -m nltk.downloader punkt stopwords wordnet\n",
    "\n",
    "\n",
    "\n",
    "# Execute main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eee30f-1107-4869-bd63-647f26405dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
